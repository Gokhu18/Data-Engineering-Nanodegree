{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import udf, col\n",
    "# from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "# config.read('dl.cfg')\n",
    "config.read('test1.cfg')\n",
    "\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_song_schema():\n",
    "    \"\"\"Build and return a schema to use for the song data.\n",
    "    Returns\n",
    "        schema: StructType object, a representation of schema and defined fields\n",
    "    \"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('artist_id', StringType(), True),\n",
    "            StructField('artist_latitude', DecimalType(), True),\n",
    "            StructField('artist_longitude', DecimalType(), True),\n",
    "            StructField('artist_location', StringType(), True),\n",
    "            StructField('artist_name', StringType(), True),\n",
    "            StructField('duration', DecimalType(), True),\n",
    "            StructField('num_songs', IntegerType(), True),\n",
    "            StructField('song_id', StringType(), True),\n",
    "            StructField('title', StringType(), True),\n",
    "            StructField('year', IntegerType(), True)\n",
    "        ]\n",
    "    )\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input_data = \"s3a://udacity-dend/\"\n",
    "\n",
    "# song_data = f'{input_data}/song_data/*/*/*/*.json'\n",
    "\n",
    "# log_data = f'{input_data}/log_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read song data file\n",
    "df_song = spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\", schema=build_song_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = df_song[['song_id', 'title', 'artist_id', 'year', 'duration']].drop_duplicates()\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.save(path='song_table',\n",
    "                       format='parquet',\n",
    "                       partitionBy=['year', 'artist_id'],\n",
    "                       mode='overwrite'\n",
    "                      )\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df_song[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']].drop_duplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.save(path='artists_table',\n",
    "                       format='parquet',\n",
    "                         mode='overwrite'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df_log_data = spark.read.json(\"s3a://udacity-dend/log-data/*/*/*.json\")\n",
    "\n",
    "# filter by actions for song plays\n",
    "df_log_data = df_log_data[df_log_data['page']=='NextSong']\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = df_log_data[['userId', 'firstName', 'lastName', 'gender', 'level']].drop_duplicates()\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.save(path='users_table',\n",
    "                       format='parquet',\n",
    "                         mode='overwrite'\n",
    "                      )\n",
    "\n",
    "df_log_data = df_log_data.withColumn('timestamp', F.from_unixtime(df_log_data['ts']/1000))\\\n",
    "                        .withColumn('hour', F.hour(F.col('timestamp')))\\\n",
    "                        .withColumn('day', F.dayofmonth(F.col('timestamp')))\\\n",
    "                        .withColumn('month', F.month(F.col('timestamp')))\\\n",
    "                        .withColumn('year', F.year(F.col('timestamp')))\\\n",
    "                        .withColumn('weekofyear', F.weekofyear(F.col('timestamp')))\\\n",
    "                        .withColumn('dayofweek', F.dayofweek(F.col('timestamp')))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df_log_data[['timestamp','hour','day','month','year','weekofyear','dayofweek',]].drop_duplicates()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.save(path='time_table',\n",
    "                      format='parquet',\n",
    "                      mode='overwrite',\n",
    "                      partitionBy=['year','month']                      )\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "df_song = spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\", schema=build_song_schema())\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df_log_data.join(df_song, \n",
    "                                   on = (df_song['title'] == df_log_data['song']) & \\\n",
    "                                       (df_song['artist_name'] == df_log_data['artist']) & \\\n",
    "                                       (df_song['duration'] == df_log_data['length'])                                  \n",
    "                                  )\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.save(path='songplays_table',\n",
    "                      format='parquet',\n",
    "                      mode='overwrite',\n",
    "                      partitionBy=['year','month']                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log_data.limit(2).write.save(path='s3a://my-sparkify-data-lake/df_log_data',\n",
    "                      format='parquet',\n",
    "                      mode='overwrite',)\n",
    "# .write.mode(\"overwrite\").parquet(\"s3a://my-sparkify-data-lake/df_log_data/parquet/myFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
